{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6861651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mpl.rcParams['figure.figsize'] = (8,6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6392bd72",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899566f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we are importing control group dataset\n",
    "directory1 = 'C:/Users/Rizwan/Desktop/DDM/empathy/data/control_group'\n",
    "\n",
    "#loading all the csv files\n",
    "files_path1 = glob.glob(directory1 + \"/*.csv\")\n",
    "all_dfss = []\n",
    "\n",
    "for file in files_path1:\n",
    "    df = pd.read_csv(file)\n",
    "    all_dfss.append(df)\n",
    "\n",
    "control_group = pd.concat(all_dfss, ignore_index=True)\n",
    "\n",
    "print(control_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a055d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "control_group.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea48914",
   "metadata": {},
   "source": [
    "we have total 157050 rows and 71 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff69ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff629c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72087f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of control_group dataframe\n",
    "df1 = control_group.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0684efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ed2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting all the datatypes with the actual data types\n",
    "\n",
    "\n",
    "# Converting these columns into datetime their unit are milisecond\n",
    "ml_date_cols = ['Recording timestamp', 'Computer timestamp', 'Recording duration', 'Eyetracker timestamp']\n",
    "\n",
    "for col in ml_date_cols:\n",
    "    df1[col] = pd.to_datetime(df1[col], unit='ms')\n",
    "\n",
    "\n",
    "# Converting these columns into datetime \n",
    "date_cols = ['Export date', 'Recording date', 'Recording date UTC', 'Recording start time', 'Recording start time UTC']\n",
    "\n",
    "for col in date_cols:\n",
    "    df1[col] = pd.to_datetime(df1[col])\n",
    "\n",
    "\n",
    "# Converting these columns into float and replace the (,) with the decimal point (.)\n",
    "\n",
    "split_cols = ['Gaze direction left X','Gaze direction left Y','Gaze direction left Z','Gaze direction right X','Gaze direction right Y','Gaze direction right Z',\n",
    "           'Pupil diameter left','Pupil diameter right','Eye position left X (DACSmm)','Eye position left Y (DACSmm)','Eye position left Z (DACSmm)',\n",
    "           'Eye position right X (DACSmm)','Eye position right Y (DACSmm)','Eye position right Z (DACSmm)','Gaze point left X (DACSmm)',\n",
    "           'Gaze point left Y (DACSmm)','Gaze point right X (DACSmm)','Gaze point right Y (DACSmm)','Gaze point X (MCSnorm)','Gaze point Y (MCSnorm)',\n",
    "           'Gaze point left X (MCSnorm)','Gaze point left Y (MCSnorm)','Gaze point right X (MCSnorm)','Gaze point right Y (MCSnorm)',\n",
    "           'Fixation point X (MCSnorm)','Fixation point Y (MCSnorm)'\n",
    "       ]\n",
    "\n",
    "for column in split_cols:\n",
    "    df1[column] =df1[column].str.replace(',', '.').astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ee557",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c72c204",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c770a5e3",
   "metadata": {},
   "source": [
    "all data types are converted to their respective data Because this is the tie series dataset so we need to set the index to Recording timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79169ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index to Recording timestamp because this is the time series dataset\n",
    "df1.set_index('Recording timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3801491",
   "metadata": {},
   "source": [
    "I did this below part in assignment 1 to remove those columns which are constant  here is the GIT HUB LINK \n",
    "https://github.com/rizwannathani/EmpathyWork/blob/02037b141959633e8d823fab2d9c3e414ee9ad58/DDM%20Assignment%201.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d2b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing these columns which has no change\n",
    "df1 = df1.drop(['Project name', 'Export date','Recording date','Computer timestamp','Recording date','Recording date UTC','Recording start time'\n",
    "                ,'Recording start time UTC','Timeline name','Event', 'Event value', 'Mouse position X', 'Mouse position Y','Recording Fixation filter name',\n",
    "                'Recording software version', 'Recording resolution height', 'Eyetracker timestamp', 'Recording resolution width', 'Recording monitor latency','Unnamed: 0'], axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788aab9e",
   "metadata": {},
   "source": [
    "### Finding the Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773bd49a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# making a list of all the nan values which are present in out dataset\n",
    "na_features = [features for features in df1.columns if df1[features].isnull().sum()>1]\n",
    "\n",
    "for features in na_features:\n",
    "    print(features, '=', np.round(df1[features].isnull().mean(),2)*100, '% Missing Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5aa25f",
   "metadata": {},
   "source": [
    "no features are having more than 80% nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are doing imputation to remove the missing data by backward and forward fill technique\n",
    "df1.interpolate(method='ffill', inplace=True)\n",
    "df1.interpolate(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check again how many total missing data we have\n",
    "\n",
    "miss_count = df1.isna().sum()\n",
    "total_na = miss_count.sum()\n",
    "print(f'Total NaN values: {total_na}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df301ed",
   "metadata": {},
   "source": [
    "### Finding numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the columns which are having numerical data\n",
    "numerical_features = [feature for feature in df1.columns if df1[feature].dtypes !='O']\n",
    "\n",
    "print('Number of Numerical Columns: ', len(numerical_features))\n",
    "\n",
    "df1[numerical_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0e62e",
   "metadata": {},
   "source": [
    "### Finding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427de0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the columns which are having non numeric data\n",
    "categorical_features=[feature for feature in df1.columns if df1[feature].dtypes=='O']\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47674c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[categorical_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding how many categories we have in a categorical columns\n",
    "for feature in categorical_features:\n",
    "    print('The feature is {} and number of categories are {}'.format(feature,len(df1[feature].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307edb67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let find the frequecny of eye type movement we have \n",
    "eye_movement_counts = df1['Eye movement type'].value_counts()\n",
    "\n",
    "plt.pie(eye_movement_counts, labels=None, autopct='%1.1f%%')\n",
    "\n",
    "plt.legend(eye_movement_counts.index, title='Eye Movement Types', loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "\n",
    "plt.title('Percentage of Eye Movement Types')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba399db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the column name\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5df871",
   "metadata": {},
   "source": [
    "Due to the complexity of the data there are so many rows for each participants respect to each trials (i.e Recording name). We will take a median of the numerical columns to deal with the outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the columns ['Recording duration'] from numerical features\n",
    "numerical_features = [col for col in numerical_features if col not in ['Recording duration']]\n",
    "\n",
    "# Create a new dataframe to store the median values with participant name and recording name\n",
    "median_df = pd.DataFrame(columns=['Participant name', 'Recording name'] + numerical_features)\n",
    "\n",
    "# Group by ['Participant name'] and each ['Recording name']\n",
    "grouped_df = df1.groupby(['Participant name', 'Recording name'])\n",
    "\n",
    "# Calculate the median of each numerical column for each group\n",
    "for name, group in grouped_df:\n",
    "    medians = [np.median(group[col]) for col in numerical_features]\n",
    "    medians.insert(0, name[0]) # Participant name\n",
    "    medians.insert(1, name[1]) # Recording name\n",
    "    median_df.loc[len(median_df)] = medians\n",
    "\n",
    "\n",
    "median_df.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69d08f",
   "metadata": {},
   "source": [
    "now we have a dataframe contain all the median values of numerical features with respect to the each participant and recording name now we do counts for the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c48c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.copy()\n",
    "# defining the categorical features \n",
    "categorical_columns = ['Sensor', 'Validity left', 'Validity right', 'Presented Stimulus name', 'Presented Media name','Eye movement type']\n",
    "\n",
    "# Define the features to group by\n",
    "group_by_columns = ['Participant name', 'Recording name']\n",
    "\n",
    "# making a new DataFrame to store the counts\n",
    "counts_df = pd.DataFrame()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    col_counts = df.groupby(group_by_columns)[col].value_counts().unstack(fill_value=0)\n",
    "    col_counts.columns = [f\"{col}_{val}\" for val in col_counts.columns]\n",
    "    counts_df = pd.concat([counts_df, col_counts], axis=1)\n",
    "\n",
    "counts_df = counts_df.reset_index()\n",
    "\n",
    "# Merge the counts_df DataFrame with the median_df DataFrame\n",
    "final_df1 = pd.merge(median_df, counts_df, on=['Participant name', 'Recording name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1688a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df1.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10bf72",
   "metadata": {},
   "source": [
    "Now we have our final dataframe which has all the median values of numerical data and the count of categorical data now we need to add a empathy score in this datafram for making of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c5e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562db581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define regular expression to extract numbers\n",
    "pattern = re.compile(r'\\d+')\n",
    "\n",
    "# Extract numbers from strings and create new column\n",
    "final_df1['Participant name'] = final_df1['Participant name'].apply(lambda x: int(pattern.search(x).group()))\n",
    "final_df1 = final_df1.rename(columns={'Participant name': 'Participant no'})\n",
    "\n",
    "\n",
    "# Output result\n",
    "final_df1.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a384a9",
   "metadata": {},
   "source": [
    "Now we need to add a empathy score to our final dataframe because we have 2 questions dataset 1A and 1B and both have different empathy score we will take an average of these 2 scores and then add the average score in our final datafarme with respect to each particpant and recording(i.e trail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1188b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions1A = pd.read_csv('C:/Users/Rizwan/Desktop/DDM/empathy/data/questions/Questionnaire_datasetIA.csv', encoding= 'unicode_escape')\n",
    "questions1B = pd.read_csv('C:/Users/Rizwan/Desktop/DDM/empathy/data/questions/Questionnaire_datasetIB.csv', encoding= 'unicode_escape')\n",
    "\n",
    "# calculate the average total score extended from both files\n",
    "avg_emp_scores = pd.concat([questions1A, questions1B])\\\n",
    "    .groupby(['Participant nr'])['Total Score extended']\\\n",
    "    .mean()\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={'Total Score extended': 'Avg Empathy score'})\n",
    "\n",
    "# create a new dataframe score_df with the average total score extended\n",
    "score_df = pd.merge(questions1A[['Participant nr']], avg_emp_scores, on='Participant nr', how='left')\n",
    "score_df = score_df.rename(columns={'Participant nr': 'Participant no'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5380f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c22922",
   "metadata": {},
   "source": [
    "Here we have got the average Empathy score with respect to each participant now will going to add this in our final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ffc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1 = final_df1.merge(score_df, on='Participant no', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f1e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638da139",
   "metadata": {},
   "source": [
    "Now we have complete one dataframe which has all the median values and counts of categorical data and also the average of the empathy score will use this dataframe to create our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c33ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving this file to csv\n",
    "final_df1.to_csv(\"finaldata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c419623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raeding that file\n",
    "finaldf1 = pd.read_csv(\"finaldata.csv\")\n",
    "finaldf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a27b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing first index\n",
    "finaldf1 = finaldf1.drop(finaldf1.columns[0], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed556d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9bac5",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f44cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target variable and features\n",
    "X = finaldf1.drop(['Avg Empathy score','Participant no','Recording name'], axis=1)\n",
    "y = finaldf1['Avg Empathy score']\n",
    "\n",
    "# Select top 15 features based on correlation with avg empathy score variable\n",
    "selector = SelectKBest(f_regression, k=15)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "mask = selector.get_support() \n",
    "selected_feat = X.columns[mask] \n",
    "\n",
    "print(\"Selected Features:\", selected_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df88bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[selected_feat]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc19c7",
   "metadata": {},
   "source": [
    "## Applying Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = finaldf1.copy()\n",
    "\n",
    "# Split data into training and testing sets will use selected_feat which we found before\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[selected_feat], df['Avg Empathy score'], test_size=0.2, random_state=42)\n",
    "\n",
    "# making multiple regression models using cross-validation\n",
    "models = {'Linear Regression': LinearRegression(),\n",
    "          'Ridge Regression': Ridge(),\n",
    "          'Lasso Regression': Lasso(),\n",
    "          'Random Forest Regression': RandomForestRegressor()}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    print(f'{name}:')\n",
    "    print(f'R-squared scores: {scores}')\n",
    "    print(f'Mean R-squared score: {scores.mean():.3f}')\n",
    "    print('')\n",
    "\n",
    "# Output the best model based on cross-validation scores\n",
    "best_model = max(models, key=lambda x: cross_val_score(models[x], X_train, y_train, cv=5, scoring='r2').mean())\n",
    "print(f'Best model: {best_model}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc3eea1",
   "metadata": {},
   "source": [
    "with the Cross validation approach for model evaluation we got Lasso Regression as the best model with Mean R-squared score of 0.689"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62865af",
   "metadata": {},
   "source": [
    "### Adding some more important features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b8fc6",
   "metadata": {},
   "source": [
    "Because in research paper it was mentioned that the pupil diameter is important so we will add pupil diameter faeture and also the count of fixation occur because fixation occurs the most in the eye movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3febfab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = finaldf1.copy()\n",
    "selected_feat1 = ['Gaze point Y', 'Gaze point left Y', 'Gaze point right Y',\n",
    "       'Gaze direction left Y', 'Gaze direction right Y',\n",
    "       'Eye position left X (DACSmm)', 'Eye position left Y (DACSmm)',\n",
    "       'Eye position right X (DACSmm)', 'Eye position right Y (DACSmm)',\n",
    "       'Gaze point left Y (DACSmm)', 'Gaze point right Y (DACSmm)',\n",
    "       'Gaze point Y (MCSnorm)', 'Gaze point right Y (MCSnorm)',\n",
    "       'Gaze event duration', 'Fixation point Y','Eye movement type_Fixation','Pupil diameter left', 'Pupil diameter right']\n",
    "\n",
    "\n",
    "# Split data into training and testing sets will use selected_feat which we found before\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[selected_feat], df['Avg Empathy score'], test_size=0.2, random_state=42)\n",
    "\n",
    "# making multiple regression models using cross-validation\n",
    "models = {'Linear Regression': LinearRegression(),\n",
    "          'Ridge Regression': Ridge(),\n",
    "          'Lasso Regression': Lasso(),\n",
    "          'Random Forest Regression': RandomForestRegressor()}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    print(f'{name}:')\n",
    "    print(f'R-squared scores: {scores}')\n",
    "    print(f'Mean R-squared score: {scores.mean():.3f}')\n",
    "    print('')\n",
    "\n",
    "# Output the best model based on cross-validation scores\n",
    "best_model = max(models, key=lambda x: cross_val_score(models[x], X_train, y_train, cv=5, scoring='r2').mean())\n",
    "print(f'Best model: {best_model}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3c0863",
   "metadata": {},
   "source": [
    "After adding ['Eye movement type_Fixation','Pupil diameter left', 'Pupil diameter right'] our model and got the same Mean R-squared score: 0.68 of lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c2a4e",
   "metadata": {},
   "source": [
    "Now we are start working on Test Group "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a654c",
   "metadata": {},
   "source": [
    "### Importing Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39bfa29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first we are importing test group dataset\n",
    "directory1 = 'C:/Users/Rizwan/Desktop/DDM/empathy/data/test_group'\n",
    "\n",
    "#loading all the csv files\n",
    "files_path1 = glob.glob(directory1 + \"/*.csv\")\n",
    "all_dfss = []\n",
    "\n",
    "for file in files_path1:\n",
    "    df = pd.read_csv(file)\n",
    "    all_dfss.append(df)\n",
    "\n",
    "test_group = pd.concat(all_dfss, ignore_index=True)\n",
    "\n",
    "print(test_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15771327",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f565359",
   "metadata": {},
   "source": [
    "we have total 211474 rows and 71 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4951ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_group.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2046eff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4193a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating a copy of control_group dataframe\n",
    "df2 = test_group.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed09254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f215c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting all the datatypes with the actual data types\n",
    "\n",
    "\n",
    "# Converting these columns into datetime their unit are milisecond\n",
    "ml_date_cols = ['Recording timestamp', 'Computer timestamp', 'Recording duration', 'Eyetracker timestamp']\n",
    "\n",
    "for col in ml_date_cols:\n",
    "    df2[col] = pd.to_datetime(df2[col], unit='ms')\n",
    "\n",
    "\n",
    "# Converting these columns into datetime \n",
    "date_cols = ['Export date', 'Recording date', 'Recording date UTC', 'Recording start time', 'Recording start time UTC']\n",
    "\n",
    "for col in date_cols:\n",
    "    df2[col] = pd.to_datetime(df2[col])\n",
    "\n",
    "\n",
    "# Converting these columns into float and replace the (,) with the decimal point (.)\n",
    "\n",
    "split_cols = ['Gaze direction left X','Gaze direction left Y','Gaze direction left Z','Gaze direction right X','Gaze direction right Y','Gaze direction right Z',\n",
    "           'Pupil diameter left','Pupil diameter right','Eye position left X (DACSmm)','Eye position left Y (DACSmm)','Eye position left Z (DACSmm)',\n",
    "           'Eye position right X (DACSmm)','Eye position right Y (DACSmm)','Eye position right Z (DACSmm)','Gaze point left X (DACSmm)',\n",
    "           'Gaze point left Y (DACSmm)','Gaze point right X (DACSmm)','Gaze point right Y (DACSmm)','Gaze point X (MCSnorm)','Gaze point Y (MCSnorm)',\n",
    "           'Gaze point left X (MCSnorm)','Gaze point left Y (MCSnorm)','Gaze point right X (MCSnorm)','Gaze point right Y (MCSnorm)',\n",
    "           'Fixation point X (MCSnorm)','Fixation point Y (MCSnorm)'\n",
    "       ]\n",
    "\n",
    "for column in split_cols:\n",
    "    df2[column] =df2[column].str.replace(',', '.').astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da087a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf86767",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f4b1f",
   "metadata": {},
   "source": [
    "all data types are converted to their respective data Because this is the tie series dataset so we need to set the index to Recording timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2620ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index to Recording timestamp because this is the time series dataset\n",
    "df2.set_index('Recording timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec0aad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c3320",
   "metadata": {},
   "source": [
    "Removing below columns I did in assigment 1 here is the link\n",
    "\n",
    "https://github.com/rizwannathani/Data-Science-and-Decision-Making/blob/63026313a95f1bc904268c27947f3c863fec2a84/DDM%20Assignment%201.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing these columns which has no change\n",
    "df2 = df2.drop(['Project name', 'Export date','Recording date','Computer timestamp','Recording date','Recording date UTC','Recording start time'\n",
    "                ,'Recording start time UTC','Timeline name','Event', 'Event value', 'Mouse position X', 'Mouse position Y','Recording Fixation filter name',\n",
    "                'Recording software version', 'Recording resolution height', 'Eyetracker timestamp', 'Recording resolution width', 'Recording monitor latency','Unnamed: 0'], axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6132a2",
   "metadata": {},
   "source": [
    "### Finding the Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c5dd8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# making a list of all the nan values which are present in out dataset\n",
    "na_features = [features for features in df2.columns if df2[features].isnull().sum()>1]\n",
    "\n",
    "for features in na_features:\n",
    "    print(features, '=', np.round(df2[features].isnull().mean(),2)*100, '% Missing Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ffe331",
   "metadata": {},
   "source": [
    "no features are having more than 80% nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fbf8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are doing imputation to remove the missing data by backward and forward fill technique\n",
    "df2.interpolate(method='ffill', inplace=True)\n",
    "df2.interpolate(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c405b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check again how many total missing data we have\n",
    "\n",
    "miss_count = df2.isna().sum()\n",
    "total_na = miss_count.sum()\n",
    "print(f'Total NaN values: {total_na}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099c7d2f",
   "metadata": {},
   "source": [
    "### Finding numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5dc7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the columns which are having numerical data\n",
    "numerical_features = [feature for feature in df2.columns if df2[feature].dtypes !='O']\n",
    "\n",
    "print('Number of Numerical Columns: ', len(numerical_features))\n",
    "\n",
    "df2[numerical_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03a474b",
   "metadata": {},
   "source": [
    "### Finding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cede91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the columns which are having non numeric data\n",
    "categorical_features=[feature for feature in df2.columns if df2[feature].dtypes=='O']\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[categorical_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding how many categories we have in a categorical columns\n",
    "for feature in categorical_features:\n",
    "    print('The feature is {} and number of categories are {}'.format(feature,len(df2[feature].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e36f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let find the frequecny of eye type movement we have \n",
    "eye_movement_counts = df2['Eye movement type'].value_counts()\n",
    "\n",
    "plt.pie(eye_movement_counts, labels=None, autopct='%1.1f%%')\n",
    "\n",
    "plt.legend(eye_movement_counts.index, title='Eye Movement Types', loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "\n",
    "plt.title('Percentage of Eye Movement Types')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec63c5c8",
   "metadata": {},
   "source": [
    "Due to the complexity of the data there are so many rows for each participants respect to each trials (i.e Recording name). We will take a median of the numerical columns to deal with the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8085aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the columns ['Recording duration'] from numerical features\n",
    "numerical_features = [col for col in numerical_features if col not in ['Recording duration']]\n",
    "\n",
    "# Create a new dataframe to store the median values with participant name and recording name\n",
    "median_df2 = pd.DataFrame(columns=['Participant name', 'Recording name'] + numerical_features)\n",
    "\n",
    "# Group by ['Participant name'] and each ['Recording name'] (i.e each trial)\n",
    "grouped_df = df2.groupby(['Participant name', 'Recording name'])\n",
    "\n",
    "# Calculate the median of each numerical column for each group\n",
    "for name, group in grouped_df:\n",
    "    medians = [np.median(group[col]) for col in numerical_features]\n",
    "    medians.insert(0, name[0]) # Participant name\n",
    "    medians.insert(1, name[1]) # Recording name\n",
    "    median_df2.loc[len(median_df2)] = medians\n",
    "\n",
    "\n",
    "median_df2.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe0932",
   "metadata": {},
   "source": [
    "now we have a dataframe contain all the median values of numerical features with respect to the each participant and recording name now we do counts for the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26738bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2.copy()\n",
    "# defining the categorical features \n",
    "categorical_columns = ['Sensor', 'Validity left', 'Validity right', 'Presented Stimulus name', 'Presented Media name','Eye movement type']\n",
    "\n",
    "# Define the features to group by\n",
    "group_by_columns = ['Participant name', 'Recording name']\n",
    "\n",
    "# making a new DataFrame to store the counts\n",
    "counts_df = pd.DataFrame()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    col_counts = df.groupby(group_by_columns)[col].value_counts().unstack(fill_value=0)\n",
    "    col_counts.columns = [f\"{col}_{val}\" for val in col_counts.columns]\n",
    "    counts_df = pd.concat([counts_df, col_counts], axis=1)\n",
    "\n",
    "counts_df = counts_df.reset_index()\n",
    "\n",
    "# Merge the counts_df DataFrame with the median_df DataFrame\n",
    "final_df2 = pd.merge(median_df2, counts_df, on=['Participant name', 'Recording name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d51f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df2.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a77da09",
   "metadata": {},
   "source": [
    "Now we have our final dataframe which has all the median values of numerical data and the count of categorical data now we need to add a empathy score in this datafram for making of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regular expression to extract numbers\n",
    "pattern = re.compile(r'\\d+')\n",
    "\n",
    "# Extract numbers from strings and create new column\n",
    "final_df2['Participant name'] = final_df2['Participant name'].apply(lambda x: int(pattern.search(x).group()))\n",
    "final_df2 = final_df2.rename(columns={'Participant name': 'Participant no'})\n",
    "\n",
    "\n",
    "# Output result\n",
    "final_df2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a1708c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_df2.head(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb91bc7",
   "metadata": {},
   "source": [
    "Now we need to add a empathy score to our final dataframe because we have 2 questions dataset 1A and 1B and both have different empathy score we will take an average of these 2 scores and then add the average score in our final datafarme with respect to each particpant and recording(i.e trail)\n",
    "\n",
    "we did this above so we just merge those empathy score to corespond participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df2 = final_df2.merge(score_df, on='Participant no', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a9db8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae898825",
   "metadata": {},
   "source": [
    "Now we have complete one dataframe which has all the median values and counts of categorical data and also the average of the empathy score will use this dataframe to create our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6324d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving this file to csv\n",
    "final_df2.to_csv(\"finaldata2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662dcaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading that file\n",
    "final_df2 = pd.read_csv(\"finaldata2.csv\")\n",
    "final_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing first index\n",
    "final_df2 = final_df2.drop(final_df2.columns[0], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b2f913",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d270828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target variable and features\n",
    "X = final_df2.drop(['Avg Empathy score','Participant no','Recording name'], axis=1)\n",
    "y = final_df2['Avg Empathy score']\n",
    "\n",
    "# Select top 15 features based on correlation with target variable\n",
    "selector = SelectKBest(f_regression, k=15)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "mask = selector.get_support()  \n",
    "selected_feat2 = X.columns[mask]  \n",
    "\n",
    "print(\"Selected Features:\", selected_feat2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36dace",
   "metadata": {},
   "source": [
    "We have selected the top 5 features which are in selected_feat2 variable we use these features to make regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e504972",
   "metadata": {},
   "source": [
    "## Applying Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe9e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df2.copy()\n",
    "\n",
    "# Split data into training and testing sets will use selected_feat which we found before\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[selected_feat2], df['Avg Empathy score'], test_size=0.2, random_state=42)\n",
    "\n",
    "# making multiple regression models using cross-validation\n",
    "models = {'Linear Regression': LinearRegression(),\n",
    "          'Ridge Regression': Ridge(),\n",
    "          'Lasso Regression': Lasso(),\n",
    "          'Random Forest Regression': RandomForestRegressor()}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    print(f'{name}:')\n",
    "    print(f'R-squared scores: {scores}')\n",
    "    print(f'Mean R-squared score: {scores.mean():.3f}')\n",
    "    print('')\n",
    "\n",
    "# Output the best model based on cross-validation scores\n",
    "best_model = max(models, key=lambda x: cross_val_score(models[x], X_train, y_train, cv=5, scoring='r2').mean())\n",
    "print(f'Best model: {best_model}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b67eb4",
   "metadata": {},
   "source": [
    "with the Cross validation approach for model evaluation we got Random Forest Regression as the best model with Mean R-squared score of 0.70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f154b59",
   "metadata": {},
   "source": [
    "### Adding some more important features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45feb491",
   "metadata": {},
   "source": [
    "Because in research paper it was mentioned that the pupil diameter is important so we will add pupil diameter faeture and also the count of fixation occur because fixation occurs the most in the eye movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb48994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cross validation\n",
    "\n",
    "df = final_df2.copy()\n",
    "selected_feature = ['Gaze point X', 'Gaze point right X', 'Gaze point right Y',\n",
    "       'Gaze direction right X', 'Eye position left X (DACSmm)',\n",
    "       'Eye position right X (DACSmm)', 'Gaze point right X (DACSmm)',\n",
    "       'Gaze point right Y (DACSmm)', 'Gaze point X (MCSnorm)',\n",
    "       'Gaze point right X (MCSnorm)', 'Gaze event duration',\n",
    "       'Fixation point X', 'Fixation point X (MCSnorm)',\n",
    "       'Presented Stimulus name_103111957_1133015250388940_5990313860353693579_n (1)',\n",
    "       'Presented Media name_103111957_1133015250388940_5990313860353693579_n.jpg',\n",
    "       'Eye movement type_Fixation','Pupil diameter left', 'Pupil diameter right']\n",
    "\n",
    "# Split data into training and testing sets will use selected_feat which we found before\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[selected_feature], df['Avg Empathy score'], test_size=0.2, random_state=42)\n",
    "\n",
    "# making multiple regression models using cross-validation\n",
    "models = {'Linear Regression': LinearRegression(),\n",
    "          'Ridge Regression': Ridge(),\n",
    "          'Lasso Regression': Lasso(),\n",
    "          'Random Forest Regression': RandomForestRegressor()}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    print(f'{name}:')\n",
    "    print(f'R-squared scores: {scores}')\n",
    "    print(f'Mean R-squared score: {scores.mean():.3f}')\n",
    "    print('')\n",
    "\n",
    "# Output the best model based on cross-validation scores\n",
    "best_model = max(models, key=lambda x: cross_val_score(models[x], X_train, y_train, cv=5, scoring='r2').mean())\n",
    "print(f'Best model: {best_model}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c7bf73",
   "metadata": {},
   "source": [
    "After adding ['Eye movement type_Fixation','Pupil diameter left', 'Pupil diameter right'] these features we increase the performance of our model and got Mean R-squared score: 0.685 of Linear Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
